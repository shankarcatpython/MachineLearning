{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeXh4+QKufjLleOmc4nB7P"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QZGEsTdDZ7B",
        "outputId": "1f6c94c3-7e50-493e-8cf0-1bf56619b651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "Hyperparamters: \n",
            " Embedding Dimension : 5 \n",
            " Window Size : 15\n",
            "Learning Rate : 0.01 \n",
            " epochs : 1000\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Model Parameters:\n",
            " Loss function - Arithmetic mean\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Vector Representation Initial Requirement: [0.76055849 0.844884   0.202172   0.96597264 0.41830429]\n",
            "Vector Representation Initial specifications: [0.02524125 0.82176181 0.38187003 0.37768834 0.87762155]\n",
            "Vector Representation Initial Design: [0.48026498 0.06361849 0.54324099 0.04011431 0.42374923]\n",
            "Vector Representation Initial blueprint: [0.30072414 0.29543874 0.25827106 0.29171076 0.23311438]\n",
            "Vector Representation Initial Coding: [0.48695263 0.02867212 0.89973396 0.98180342 0.71614756]\n",
            "Vector Representation Initial functional: [0.94788655 0.19347953 0.97079239 0.68656925 0.67964766]\n",
            "Vector Representation Initial code: [0.10260032 0.21900278 0.06637066 0.27076148 0.77782259]\n",
            "Vector Representation Initial Testing: [0.21325677 0.804299   0.95645538 0.79066329 0.70926924]\n",
            "Vector Representation Initial examination: [0.12951306 0.78275802 0.25957367 0.30664958 0.30289254]\n",
            "Vector Representation Initial Deployment: [0.57308911 0.68687208 0.74682971 0.14880069 0.29950553]\n",
            "Vector Representation Initial finalized: [0.56432918 0.61306371 0.58608372 0.15003773 0.98050519]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Vector representation Final Requirement: [0.3947474  0.833381   0.29156951 0.67330859 0.64680885]\n",
            "Vector representation Final specifications: [0.3947474  0.833381   0.29156951 0.67330859 0.64680886]\n",
            "Vector representation Final Design: [0.39094567 0.17894615 0.40147203 0.16528038 0.32891079]\n",
            "Vector representation Final blueprint: [0.39094567 0.17894615 0.40147203 0.16528038 0.32891079]\n",
            "Vector representation Final Coding: [0.51170774 0.14677539 0.64551339 0.64687268 0.72460041]\n",
            "Vector representation Final functional: [0.51170787 0.14677543 0.6455134  0.64687259 0.7246004 ]\n",
            "Vector representation Final code: [0.51170763 0.14677544 0.64551315 0.64687247 0.72460042]\n",
            "Vector representation Final Testing: [0.17159532 0.79358263 0.60976549 0.54987255 0.50710194]\n",
            "Vector representation Final examination: [0.17159532 0.79358263 0.60976548 0.54987255 0.50710193]\n",
            "Vector representation Final Deployment: [0.56873116 0.65015334 0.6668606  0.1494161  0.63829431]\n",
            "Vector representation Final finalized: [0.56873116 0.65015334 0.6668606  0.1494161  0.63829431]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class WordEmbedding:\n",
        "    # Initialize the weights of Model\n",
        "    def __init__(self, vocab, embedding_dim):\n",
        "        self.vocab = vocab\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embeddings = {word: np.random.rand(embedding_dim) for word in vocab}\n",
        "\n",
        "    # Update Embeddings Learning process - Ignore non-keys\n",
        "    def update_embeddings(self, target_word, context_words, learning_rate=0.01):\n",
        "\n",
        "       #print(target_word  , context_words)\n",
        "        target_vector = self.embeddings.get(target_word)\n",
        "        context_vectors = []\n",
        "        for context_word in context_words:\n",
        "            context_vector = self.embeddings.get(context_word)\n",
        "            if context_vector is not None:\n",
        "              context_vectors.append(context_vector)\n",
        "\n",
        "        #print(target_vector  , context_vectors)\n",
        "\n",
        "        if target_vector is not None and all(context_vector is not None for context_vector in context_vectors):\n",
        "            target_vector_arr = np.array(target_vector)\n",
        "            context_vectors_arr = np.array(context_vectors)\n",
        "\n",
        "            # Move target vector closer to context vectors\n",
        "            # Model paramter - Mean is chosen to minimize loss ( * also called as loss function)\n",
        "            #print(target_word , \"Initial\", self.embeddings[target_word])\n",
        "            self.embeddings[target_word] = target_vector_arr + learning_rate * (np.mean(context_vectors_arr, axis=0) - target_vector_arr)\n",
        "            #print(target_word , \"Adjusted\", self.embeddings[target_word])\n",
        "\n",
        "    # Update Embeddings Learning process - ignore non-keys\n",
        "    def train(self, corpus, window_size, learning_rate, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            for sentence in corpus:\n",
        "                for i, target_word in enumerate(sentence):\n",
        "                    context = self.get_context_words(sentence, i, window_size)\n",
        "                    self.update_embeddings(target_word, context, learning_rate)\n",
        "\n",
        "    # Traverse corpus sentences within specified window , identify target context ( * collection of words )\n",
        "    def get_context_words(self, sentence, target_index, window_size):\n",
        "        start = max(0, target_index - window_size)\n",
        "        end = min(len(sentence), target_index + window_size + 1)\n",
        "        context = [sentence[i] for i in range(start, end) if i != target_index]\n",
        "        return context\n",
        "\n",
        "    def get_word_vector(self, word):\n",
        "        return self.embeddings.get(word)\n",
        "\n",
        "# Software Engineering embedding vocabulary , corpus , Tockenized Input / Output\n",
        "\n",
        "vocab = [\"Requirement\",\"specifications\", \"Design\", \"blueprint\",\"Coding\",\"functional\", \"code\" , \"Testing\", \"examination\",\"Deployment\",\"finalized\"]\n",
        "corpus = [[\"Clear\", \"and\", \"concise\", \"specifications\", \"pave\", \"the\", \"path\", \"for\", \"successful\", \"project\", \"endeavors.\", \"Requirement\"],\n",
        "    [\"Crafting\", \"an\", \"elegant\", \"blueprint\", \"ensures\", \"the\", \"foundation\", \"for\", \"a\", \"robust\", \"and\", \"scalable\", \"solution.\", \"Design\"],\n",
        "    [\"Transforming\", \"conceptualized\", \"designs\", \"into\", \"functional\", \"code\", \"demands\", \"precision\", \"and\", \"creativity.\", \"Coding\"],\n",
        "    [\"Rigorous\", \"examination\", \"guarantees\", \"the\", \"reliability\", \"and\", \"functionality\", \"of\", \"the\", \"developed\", \"system.\", \"Testing\"],\n",
        "    [\"Seamlessly\", \"rolling\", \"out\", \"the\", \"finalized\", \"product\", \"heralds\", \"the\", \"culmination\", \"of\", \"diligent\", \"efforts\", \"and\", \"signifies\", \"the\", \"beginning\", \"of\", \"its\", \"journey.\", \"Deployment\"]\n",
        "]\n",
        "\n",
        "# Hyperparameters Used in Model\n",
        "embedding_dim = 5  # Adjust as needed\n",
        "model = WordEmbedding(vocab, embedding_dim)\n",
        "window_size = 15\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "\n",
        "print(\"-\" * 100)\n",
        "print(f\"Hyperparamters: \\n Embedding Dimension : {embedding_dim} \\n Window Size : {window_size}\")\n",
        "print(f\"Learning Rate : {learning_rate} \\n epochs : {epochs}\")\n",
        "print(\"-\" * 100)\n",
        "print(f\"Model Parameters:\\n Loss function - Arithmetic mean\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "# Initial Weights\n",
        "for value in vocab:\n",
        "    word_vector = model.get_word_vector(value)\n",
        "    print(f\"Vector Representation Initial {value}:\", word_vector)\n",
        "\n",
        "# Train the model\n",
        "model.train(corpus, window_size, learning_rate, epochs)\n",
        "\n",
        "print(\"-\" * 100)\n",
        "\n",
        "# Final weights\n",
        "for value in vocab:\n",
        "    word_vector = model.get_word_vector(value)\n",
        "    print(f\"Vector representation Final {value}:\", word_vector)"
      ]
    }
  ]
}